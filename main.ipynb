{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from functools import partial\n",
    "sys.path.append(\"/home/kai/muku_esg/ESG-BERT/\")\n",
    "from allgrammer import grammer\n",
    "from pdfminer.high_level import extract_text\n",
    "%cd /home/kai/muku_esg/ESG-BERT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', '', text)\n",
    "    text = re.sub(r'[\\x0C]', r\"\\n\\n\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'taiwan_pdf'\n",
    "csv_folder = 'predict_taiwan'\n",
    "word_cnt = 'eight_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = 'american_pdf'\n",
    "# csv_folder = 'predict_american'\n",
    "# word_cnt = 'eight_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, folder, file_name):\n",
    "    x = x.replace('\\n', ' ')\n",
    "    x = re.sub(r'[!\\\"#$%&\\'()*\\+,-.\\/:;<=>?@\\[\\\\\\]^_`{|}~]', '', x)\n",
    "    x = x.replace('\\t', ' ')\n",
    "    x = x.strip()\n",
    "    if len(x.split()) < 8:\n",
    "        return None\n",
    "    else:\n",
    "        return grammer(x, folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['15_msi_1_97.pdf',\n",
    "   '16_gigabyte_1_74.pdf', '17_nanya_1_154.pdf', '18_novatek_1_140.pdf', '19_mediatek_1_97.pdf', '20_asia_1_119.pdf', \n",
    "   '21_cathay_1_52.pdf', '22_cht_1_79.pdf', '23_fubon_1_89.pdf', '24_mega_1_68.pdf']\n",
    "for file_name in file_names:\n",
    "    file_name = file_name.strip('.pdf')\n",
    "    os.chdir('/home/kai/muku_esg/ESG-BERT')\n",
    "    text = extract_text(f\"./{folder}/{file_name}.pdf\")\n",
    "    text = ascii(text)\n",
    "    text = text.strip('\\\"')\n",
    "    paragraph = re.split(r'\\n\\s*\\n', text)\n",
    "    constant_preprocess = partial(preprocess, folder=folder, file_name=file_name)\n",
    "    paragraph = map(constant_preprocess, paragraph)\n",
    "    paragraph = list(paragraph)\n",
    "    y = []\n",
    "    for i in range(len(paragraph)):\n",
    "        if paragraph[i] != None:\n",
    "            x = paragraph[i].split()\n",
    "            fifthlist = x[:300]\n",
    "            fifthstr = ' '.join(fifthlist)\n",
    "            y.append(fifthstr)\n",
    "    paragraph = y\n",
    "    #   predict\n",
    "    os.chdir('./predict_american')\n",
    "    with open(\"./output.txt\", \"w\") as file:\n",
    "        file.truncate()\n",
    "    with open(\"./paragraph.txt\", \"w\") as file:\n",
    "        file.truncate()\n",
    "    for i in range(len(paragraph)):\n",
    "        # Open a file in write mode\n",
    "        with open(\"./paragraph.txt\", \"w\") as f:\n",
    "            # Write the strings to the file\n",
    "            f.writelines(paragraph[i])\n",
    "        os.system(f'curl -X POST http://127.0.0.1:8080/predictions/bert -T paragraph.txt >> output.txt')\n",
    "        os.system(f'echo >> output.txt')\n",
    "    #   read output label and create csv\n",
    "    with open('./output.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "        labels = [x.strip('\\n') for x in labels]\n",
    "    ans = pd.DataFrame({'paragraph': paragraph, 'predict_label': labels})\n",
    "    ans.to_csv(f'../{csv_folder}/{word_cnt}/{file_name}.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the predict_label to csv\n",
    "os.chdir('/home/kai/muku_esg/ESG-BERT/predict_taiwan/five_word')\n",
    "file_name = '08_witron_1_93'\n",
    "with open(f'./{file_name}.csv', 'r') as f:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(f)\n",
    "    paragraphs = []\n",
    "    labels = []\n",
    "    for row in reader:\n",
    "        if row[0][:27] == '2021 Wistron ITS ESG Report' :\n",
    "            pass\n",
    "        else:\n",
    "            paragraphs.append(row[0])\n",
    "            labels.append(row[1])\n",
    "    ans = pd.DataFrame({'paragraph': paragraphs, 'predict_label': labels})\n",
    "    ans.to_csv(f'../five_new/{file_name}.csv', sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esgbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ab08fa76665a89038b50b331dd9223d4e470d2d4facccdbf60961d882d1011"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
